{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Intervention Responsiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "import warnings\n",
    "from copy import deepcopy\n",
    "import joblib \n",
    "from itertools import product, combinations, chain\n",
    "\n",
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy.stats import norm, mannwhitneyu\n",
    "\n",
    "# Machine learning models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Preprocessing and model evaluation\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, f1_score, recall_score,\n",
    "    precision_recall_curve, confusion_matrix, auc, precision_score,\n",
    "    balanced_accuracy_score, matthews_corrcoef\n",
    ")\n",
    "\n",
    "# Model interpretation\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEED = 123\n",
    "SEED = 321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define threshold for responsiveness\n",
    "\n",
    "Indicate change threshold that qualifies a participant as responsive vs non-responsive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE RESPONSIVENESS\n",
    "# avg reduction in drinking occasions between active and control weeks\n",
    "def_response_drink_occasions = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"../../results\"\n",
    "\n",
    "data_study1 = pd.read_csv('../../SHINE/osf_study1.csv')\n",
    "data_study2 = pd.read_csv('../../SHINE/osf_study2.csv')\n",
    "\n",
    "# Study 1 baseline data (train/val input)\n",
    "b1_alcohol_self = pd.read_csv('../../SHINE/final_buckets/alcoholself_bucket280225.csv', index_col=0)\n",
    "b2_group_subjective = pd.read_csv('../../SHINE/final_buckets/subjective_grouperceptions_280225.csv', index_col=0)\n",
    "b3_group_sociometric = pd.read_csv('../../src/responsiveness/data_social.csv')\n",
    "b4_brain = pd.read_csv('../../SHINE/final_buckets/brain_bucket_280225.csv', index_col=0)\n",
    "b5_demographic = pd.read_csv('../../SHINE/final_buckets/demographic_bucket280225.csv', index_col=0)\n",
    "b6_psychometric = pd.read_csv('../../SHINE/final_buckets/psychometrics_bucket280225.csv', index_col=0)\n",
    "\n",
    "# Study 2 subjective data (test input)\n",
    "b2_group_subjective_study2 = pd.read_csv('/Users/fmagdalena/Documents/GitHub/shine-network-analysis/SHINE/final_buckets/subjective_grouperceptions_test.csv')\n",
    "\n",
    "# Study 1 & 2 drinking/responsiveness data (output -> prediction target)\n",
    "\n",
    "if def_response_drink_occasions == -1:\n",
    "    responsive_study1 = pd.read_csv('../../SHINE/final_buckets/responsiveness_study1.csv', index_col=0).reset_index()\n",
    "# elif def_response_drink_occasions == -0.5:\n",
    "#     responsive_study1 = pd.read_csv('../../SHINE/final_buckets/responsiveness_study1_-0.5.csv', index_col=0).reset_index()\n",
    "# elif def_response_drink_occasions == -2:\n",
    "#     responsive_study1 = pd.read_csv('../../SHINE/final_buckets/responsiveness_study1_-2.csv', index_col=0).reset_index()\n",
    "\n",
    "responsive_study2 = pd.read_csv('../../SHINE/final_buckets/responsiveness_study2.csv', index_col=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_study1_control = data_study1[data_study1.condition == 'control']\n",
    "data_study2_control = data_study2[data_study2.condition == 'control']\n",
    "\n",
    "len(data_study1_control)\n",
    "len(data_study2_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates within each DataFrame\n",
    "duplicates_study1 = responsive_study1['id'].duplicated().any()\n",
    "duplicates_study2 = responsive_study2['id'].duplicated().any()\n",
    "\n",
    "print(f\"Study 1 has duplicates: {duplicates_study1}\")\n",
    "print(f\"Study 2 has duplicates: {duplicates_study2}\")\n",
    "\n",
    "# Check for overlapping IDs between the two studies\n",
    "ids_study1 = set(responsive_study1['id'])\n",
    "ids_study2 = set(responsive_study2['id'])\n",
    "overlap = ids_study1.intersection(ids_study2)\n",
    "\n",
    "print(f\"Number of overlapping IDs: {len(overlap)}\")\n",
    "if overlap:\n",
    "    print(f\"Overlapping IDs: {overlap}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE_VARS = [\n",
    "    'group', 'condition', 'active',\n",
    "    'control', 'difference_drinks_occasions']\n",
    "\n",
    "responsive_study1.drop(columns=EXCLUDE_VARS, inplace=True)\n",
    "responsive_study2.drop(columns=EXCLUDE_VARS, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responsive_study2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Baseline and Target Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training datasets -> Study 1\n",
    "b1_alcohol_self_response = pd.merge(b1_alcohol_self, responsive_study1, on='id', how='inner')\n",
    "b2_group_subjective_response = pd.merge(b2_group_subjective, responsive_study1, on='id', how='inner')\n",
    "b2_group_subjective_response_old = pd.merge(responsive_study1, responsive_study1, on='id', how='inner')\n",
    "b3_group_sociometric_response = pd.merge(b3_group_sociometric, responsive_study1, on='id', how='inner')\n",
    "b4_brain_response = pd.merge(b4_brain, responsive_study1, on='id', how='inner')\n",
    "b5_demographic_response = pd.merge(b5_demographic, responsive_study1, on='id', how='inner')\n",
    "b6_psychometric_response = pd.merge(b6_psychometric, responsive_study1, on='id', how='inner')\n",
    "\n",
    "print(f'Total IDs Study 1: {len(b1_alcohol_self_response)}')\n",
    "print(f'Responsive IDs Study 1: {b1_alcohol_self_response[b1_alcohol_self_response[\"responsive\"] == 1][\"id\"].nunique()}')\n",
    "print('----------')\n",
    "# Testing dataset -> Study 2\n",
    "b2_group_subjective_test = pd.merge(b2_group_subjective_study2, responsive_study2, on='id', how='inner')\n",
    "print(f'Total IDs Study 2: {len(b2_group_subjective_test)}')\n",
    "print(f'Responsive IDs Study 2: {b2_group_subjective_test[b2_group_subjective_test[\"responsive\"] == 1][\"id\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    'alc_self': b1_alcohol_self_response,\n",
    "    'group_sub': b2_group_subjective_response,\n",
    "    'group_socio': b3_group_sociometric_response,\n",
    "    'brain': b4_brain_response,\n",
    "    'demo': b5_demographic_response,\n",
    "    'psych': b6_psychometric_response\n",
    "}\n",
    "\n",
    "for key, df in dataframes.items():\n",
    "    print(f\"Missing values in '{key}':\")\n",
    "    print(df.isna().sum())\n",
    "    print()  # for spacing between outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find highly correlated features within buckets\n",
    "Find redundancy in features if they are highly correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    'alc_self': b1_alcohol_self_response,\n",
    "    'group_sub': b2_group_subjective_response,\n",
    "    'group_socio': b3_group_sociometric_response,\n",
    "    'brain': b4_brain_response,\n",
    "    'demo': b5_demographic_response,\n",
    "    'psych': b6_psychometric_response\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2_group_subjective_response.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_VAR = 'responsive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_highly_correlated_features(dataframes, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Identifies pairs of highly correlated features in each dataframe.\n",
    "    :param dataframes: dict of {name: dataframe}\n",
    "    :param threshold: correlation threshold to consider as \"high\"\n",
    "    :return: dict of {name: list of correlated feature pairs}\n",
    "    \"\"\"\n",
    "    correlated_features = {}\n",
    "    for name, df in dataframes.items():\n",
    "        # Exclude COMMON_VARS from the correlation computation\n",
    "        columns_to_correlate = [col for col in df.columns if col != TARGET_VAR and col !='id']\n",
    "        \n",
    "        # Compute correlation matrix only for selected columns\n",
    "        corr_matrix = df[columns_to_correlate].corr().abs()\n",
    "        \n",
    "        # Select the upper triangle of the correlation matrix\n",
    "        upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        \n",
    "        # Find pairs of features with correlation above the threshold\n",
    "        correlated_pairs = [\n",
    "            (col, idx, upper_triangle.loc[idx, col])\n",
    "            for col in upper_triangle.columns\n",
    "            for idx in upper_triangle.index\n",
    "            if upper_triangle.loc[idx, col] > threshold\n",
    "        ]\n",
    "        \n",
    "        # Store results for the current dataframe\n",
    "        correlated_features[name] = correlated_pairs\n",
    "\n",
    "    return correlated_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_features = find_highly_correlated_features(dataframes, threshold=0.8)\n",
    "\n",
    "# Display results\n",
    "for name, pairs in correlated_features.items():\n",
    "    print(f\"\\n{name} - Highly Correlated Features:\")\n",
    "    for col1, col2, corr_value in pairs:\n",
    "        print(f\"  {col1} ↔ {col2} : Correlation = {corr_value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove highly correlated features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choice is made manually \n",
    "\n",
    "dataframes['brain'].drop(columns=['reward', 'ROI_alc_react_v_rest_neurosynth_cogcontrol', 'ROI_alc_react_v_rest_neurosynth_craving', \\\n",
    "                                  'ROI_alc_react_v_rest_neurosynth_emoreg'], inplace=True)\n",
    "\n",
    "dataframes['group_socio'].drop(columns=['leaders_deg_in', 'goToBad_deg_in'], inplace=True)\n",
    "\n",
    "dataframes['psych'].drop(columns=['ACS_focus', 'DERS_strategies', 'BIS_attention_total'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all within-category correlations are gone\n",
    "correlated_features = find_highly_correlated_features(dataframes, threshold=0.8)\n",
    "\n",
    "for name, pairs in correlated_features.items():\n",
    "    print(f\"\\n{name} - Highly Correlated Features:\")\n",
    "    for col1, col2, corr_value in pairs:\n",
    "        print(f\"  {col1} ↔ {col2} : Correlation = {corr_value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of features per category\n",
    "{key: df.shape[1] for key, df in dataframes.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance tests: Features\n",
    "### Mann-Whitney U Tests\n",
    "\n",
    "Hypothesis test for non-normally distributed data to check which of the remaining features show the most (significant) difference between the two groups (responsive vs non-responsive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    'alc_self': b1_alcohol_self_response,\n",
    "    'group_sub': b2_group_subjective_response,\n",
    "    'group_socio': b3_group_sociometric_response,\n",
    "    'brain': b4_brain_response,\n",
    "    'demo': b5_demographic_response,\n",
    "    'psych': b6_psychometric_response\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_mann_whitney_u(df, target_var, exclude_vars):\n",
    "    results = {}\n",
    "    for col in df.columns:\n",
    "        if col not in exclude_vars and col != target_var:\n",
    "            try:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                group1 = df[df[target_var] == 0][col]\n",
    "                group2 = df[df[target_var] == 1][col]\n",
    "                stat, p_value = mannwhitneyu(group1, group2, alternative='two-sided')\n",
    "                results[col] = {'U_statistic': stat, 'p_value': p_value}\n",
    "            except Exception as e:\n",
    "                results[col] = {'error': str(e)}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mwu_results = {}\n",
    "for name, df in dataframes.items():\n",
    "    if name != 'demo' and TARGET_VAR in df.columns:\n",
    "        mwu_results[name] = perform_mann_whitney_u(df, TARGET_VAR, 'id')\n",
    "\n",
    "# Output summary\n",
    "for name, results in mwu_results.items():\n",
    "    print(f\"\\n{name} DataFrame Mann-Whitney U Test Results (p-value < 0.05):\")\n",
    "    \n",
    "    # Ensure only variables with p-values < 0.05 are retained\n",
    "    significant_results = {}\n",
    "    for var, stats in results.items():\n",
    "        if isinstance(stats, dict) and 'p_value' in stats and stats['p_value'] < 0.05:\n",
    "            significant_results[var] = stats  \n",
    "    \n",
    "    if significant_results:\n",
    "        df_significant = pd.DataFrame(significant_results).T  \n",
    "        print(df_significant)\n",
    "    else:\n",
    "        print(\"No significant results (p-value < 0.05) found.\")\n",
    "    print(\"---------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_features_and_targets(df, test_set=0, resampling=None):\n",
    "    if TARGET_VAR not in df.columns:\n",
    "        raise ValueError(f\"Target variable '{TARGET_VAR}' not found in dataframe.\")\n",
    "\n",
    "    # Extract target variable and drop excluded columns\n",
    "    targets = df[TARGET_VAR]\n",
    "    features = df[[col for col in df.columns if col != TARGET_VAR and col != 'id']]\n",
    "    features = features.drop(columns=[TARGET_VAR], errors='ignore')\n",
    "\n",
    "    # Split into training and test sets (STRATIFIED)\n",
    "    if test_set:\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            features, targets, test_size=test_set, stratify=targets\n",
    "        )\n",
    "    else: \n",
    "        X_train = features\n",
    "        Y_train = targets\n",
    "        X_test = []\n",
    "        Y_test = []\n",
    "\n",
    "    # Median imputation for 'income_numeric' if it contains NA values\n",
    "    if 'income_numeric' in X_train.columns:\n",
    "        if X_train['income_numeric'].isna().any():\n",
    "            X_train['income_numeric'].fillna(X_train['income_numeric'].median(), inplace=True)\n",
    "        if isinstance(X_test, pd.DataFrame) and 'income_numeric' in X_test.columns and X_test['income_numeric'].isna().any():\n",
    "            X_test['income_numeric'].fillna(X_test['income_numeric'].median(), inplace=True)\n",
    "\n",
    "    if 'IAS_mean' in X_train.columns:\n",
    "        if X_train['IAS_mean'].isna().any():\n",
    "            X_train['IAS_mean'].fillna(X_train['IAS_mean'].median(), inplace=True)\n",
    "        if isinstance(X_test, pd.DataFrame) and 'IAS_mean' in X_test.columns and X_test['IAS_mean'].isna().any():\n",
    "            X_test['IAS_mean'].fillna(X_test['IAS_mean'].median(), inplace=True)\n",
    "\n",
    "    # TODO: Handle all other missingness here\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_kfold_grid_search(\n",
    "    X, Y, param_grid, k=5, CV_reps=1, eval_metric=['auc'], model_choice_metric='auc', \n",
    "    res_dir=\".\", model_type='rf', combo='alcohol'\n",
    "):\n",
    "\n",
    "    # Generate all parameter combinations\n",
    "    param_combinations = list(product(*param_grid.values()))\n",
    "    param_names = list(param_grid.keys())\n",
    "\n",
    "    # Initialize variables to store the best model and scores\n",
    "    best_model = None\n",
    "    best_scores = None\n",
    "    best_params = None\n",
    "    best_model_choice_value = -np.inf  # Track the best model based on the chosen metric\n",
    "\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True)\n",
    "\n",
    "    for params in param_combinations:\n",
    "        current_params = dict(zip(param_names, params))\n",
    "\n",
    "        # Store all fold results\n",
    "        all_folds_metrics = {metric: [] for metric in eval_metric}\n",
    "\n",
    "        for train_index, test_index in kf.split(X, Y):  # k-fold cv split\n",
    "\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "\n",
    "            rep_metrics = {metric: [] for metric in eval_metric}  # Reset for each fold\n",
    "\n",
    "            for _ in range(CV_reps):  # Repeat that split j times\n",
    "\n",
    "                # Initialize the model with the current parameters\n",
    "                if model_type == 'rf':\n",
    "                    model = RandomForestClassifier(\n",
    "                        n_estimators=current_params.get(\"n_estimators\", 100),\n",
    "                        max_depth=current_params.get(\"max_depth\"),\n",
    "                        min_samples_split=current_params.get(\"min_samples_split\", 2),\n",
    "                        min_samples_leaf=current_params.get(\"min_samples_leaf\", 1),\n",
    "                        class_weight=\"balanced\"\n",
    "                    )\n",
    "                elif model_type == 'xgb':\n",
    "                    model = XGBClassifier(\n",
    "                        n_estimators=current_params.get(\"n_estimators\", 100),\n",
    "                        max_depth=current_params.get(\"max_depth\", 6),\n",
    "                        learning_rate=current_params.get(\"learning_rate\", 0.1),\n",
    "                        min_child_weight=current_params.get(\"min_child_weight\", 1),\n",
    "                        gamma=current_params.get(\"gamma\", 0),\n",
    "                        subsample=current_params.get(\"subsample\", 1),\n",
    "                        colsample_bytree=current_params.get(\"colsample_bytree\", 1),\n",
    "                        scale_pos_weight=current_params.get(\"scale_pos_weight\", 1),\n",
    "                        use_label_encoder=False,\n",
    "                        eval_metric=\"logloss\"\n",
    "                    )\n",
    "\n",
    "                model.fit(X_train, Y_train)\n",
    "                Y_pred = model.predict(X_test)\n",
    "                Y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "\n",
    "                if 'auc' in eval_metric and Y_prob is not None:\n",
    "                    rep_metrics['auc'].append(roc_auc_score(Y_test, Y_prob))\n",
    "                if 'f1' in eval_metric:\n",
    "                    rep_metrics['f1'].append(f1_score(Y_test, Y_pred))\n",
    "                if 'accuracy' in eval_metric:\n",
    "                    rep_metrics['accuracy'].append(accuracy_score(Y_test, Y_pred))\n",
    "                if 'specificity' in eval_metric or 'sensitivity' in eval_metric:\n",
    "                    tn, fp, fn, tp = confusion_matrix(Y_test, Y_pred).ravel()\n",
    "                    specificity = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "                    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "                    if 'specificity' in eval_metric:\n",
    "                        rep_metrics['specificity'].append(specificity)\n",
    "                    if 'sensitivity' in eval_metric:\n",
    "                        rep_metrics['sensitivity'].append(sensitivity)\n",
    "                if 'mcc' in eval_metric:\n",
    "                    rep_metrics['mcc'].append(matthews_corrcoef(Y_test, Y_pred))\n",
    "                if 'balancedAcc' in eval_metric:\n",
    "                    rep_metrics['balancedAcc'].append(balanced_accuracy_score(Y_test, Y_pred))\n",
    "                if 'pr_auc' in eval_metric and Y_prob is not None:\n",
    "                    precision, recall, _ = precision_recall_curve(Y_test, Y_prob)\n",
    "                    pr_auc = auc(recall, precision)\n",
    "                    rep_metrics['pr_auc'].append(pr_auc)\n",
    "\n",
    "            # Compute median scores per fold and store results\n",
    "            fold_median_metrics = {metric: np.mean(values) for metric, values in rep_metrics.items()}\n",
    "            for metric in eval_metric:\n",
    "                all_folds_metrics[metric].append(fold_median_metrics[metric])\n",
    "\n",
    "        # Compute final median scores over all folds\n",
    "        median_rep_metrics = {metric: np.mean(values) for metric, values in all_folds_metrics.items()}\n",
    "\n",
    "        # Select best model based on median of model_choice_metric\n",
    "        if median_rep_metrics[model_choice_metric] > best_model_choice_value:\n",
    "            best_model_choice_value = median_rep_metrics[model_choice_metric]\n",
    "            best_model = model\n",
    "            best_params = current_params\n",
    "            best_scores = median_rep_metrics  # Store median scores for all metrics\n",
    "\n",
    "    return best_model, best_scores, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics_to_csv(results_dict, results_dir, filename):\n",
    "\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Define the output file path\n",
    "    file_path = os.path.join(results_dir, filename)\n",
    "    \n",
    "    # Extract all metric names\n",
    "    all_metrics = set()\n",
    "    for metrics in results_dict.values():\n",
    "        all_metrics.update(metrics.keys())\n",
    "    \n",
    "    # Sort metrics for consistency\n",
    "    all_metrics = sorted(all_metrics)\n",
    "\n",
    "    # Open CSV file for writing\n",
    "    with open(file_path, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write header\n",
    "        header = [\"run\", \"group\"] + all_metrics\n",
    "        writer.writerow(header)\n",
    "\n",
    "        # Write data\n",
    "        for group, metrics in results_dict.items():\n",
    "            num_runs = len(next(iter(metrics.values())))  # Get number of runs from first metric\n",
    "            for run_idx in range(num_runs):\n",
    "                row = [run_idx, str(group)]  # Start with run index and group name\n",
    "                for metric in all_metrics:\n",
    "                    value = metrics.get(metric, [np.nan] * num_runs)[run_idx]  # Handle missing values\n",
    "                    row.append(value)\n",
    "                writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_metrics(Y_test, test_predictions, proba_predictions, test_scores):\n",
    "    Y_test_flat = Y_test.ravel()\n",
    "    \n",
    "    test_scores['auc'].append(roc_auc_score(Y_test_flat, proba_predictions))\n",
    "    test_scores['f1'].append(f1_score(Y_test_flat, test_predictions))\n",
    "    test_scores['accuracy'].append(accuracy_score(Y_test_flat, test_predictions))\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(Y_test_flat, test_predictions).ravel()\n",
    "    test_scores['specificity'].append(tn / (tn + fp) if (tn + fp) > 0 else np.nan)\n",
    "    test_scores['sensitivity'].append(tp / (tp + fn) if (tp + fn) > 0 else np.nan)\n",
    "    test_scores['PPV'].append(tp / (tp + fp) if (tp + fp) > 0 else np.nan)\n",
    "    test_scores['NPV'].append(tn / (tn + fn) if (tn + fn) > 0 else np.nan)\n",
    "    test_scores['MCC'].append(matthews_corrcoef(Y_test_flat, test_predictions))\n",
    "    test_scores['balancedAcc'].append(balanced_accuracy_score(Y_test_flat, test_predictions))\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(Y_test_flat, proba_predictions)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    test_scores['pr_auc'].append(pr_auc)\n",
    "\n",
    "    test_scores['tn'].append(tn)\n",
    "    test_scores['fp'].append(fp)\n",
    "    test_scores['fn'].append(fn)\n",
    "    test_scores['tp'].append(tp)\n",
    "\n",
    "    return test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_score_dict(score_dict, res_dir=None, filename=None):\n",
    "    rows = []\n",
    "    for combination, metrics in score_dict.items():\n",
    "        row = {\"Combination\": combination}\n",
    "        for metric, values in metrics.items():\n",
    "            row[f\"{metric}_mean\"] = values[\"mean\"]\n",
    "            row[f\"{metric}_CI_lower\"] = values[\"95%_CI\"][0]\n",
    "            row[f\"{metric}_CI_upper\"] = values[\"95%_CI\"][1]\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df_comb = pd.DataFrame(df[\"Combination\"].tolist(), columns=[f\"Factor_{i+1}\" for i in range(df[\"Combination\"].map(len).max())])\n",
    "    df = pd.concat([df_comb, df.drop(columns=\"Combination\")], axis=1)\n",
    "\n",
    "    if res_dir and filename:\n",
    "        df.to_csv(f\"{res_dir}/{filename}\", index=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_summary_with_percentages(all_shap_values, all_test_data, res_dir, combo):\n",
    "    # Mapping of original to preferred variable names\n",
    "    name_mapping = {\n",
    "        \"avg_alcmost\": \"Peer Perception: Drinking Amount\",\n",
    "        \"groupAtt_alc\": \"Peer Attitudes: Alcohol\",\n",
    "        \"avg_alcmost_freq\": \"Peer Perception: Drinking Frequency\",\n",
    "        \"alc_norm_5_r\": \"Perceived Peer Pressure\",\n",
    "        \"groupAtt_binge\": \"Peer Attitudes: Binges\"\n",
    "    }\n",
    "\n",
    "    # Combine SHAP values and test data\n",
    "    final_shap_values = np.vstack(all_shap_values)\n",
    "    final_test_data = pd.concat(all_test_data, ignore_index=True)\n",
    "\n",
    "    # Compute relative importance\n",
    "    mean_abs_shap = np.abs(final_shap_values).mean(axis=0)\n",
    "    rel_importance = 100 * mean_abs_shap / mean_abs_shap.sum()\n",
    "\n",
    "    # Plot SHAP summary without showing\n",
    "    plt.figure()\n",
    "    shap.summary_plot(final_shap_values, final_test_data, show=False, cmap='winter')\n",
    "\n",
    "    # Get current axis and y-tick labels\n",
    "    ax = plt.gca()\n",
    "    feature_names = [tick.get_text() for tick in ax.get_yticklabels()]\n",
    "\n",
    "    # Map to preferred names if available\n",
    "    mapped_feature_names = [name_mapping.get(name, name) for name in feature_names]\n",
    "\n",
    "    # Use Index.get_loc instead of list\n",
    "    col_index = final_test_data.columns\n",
    "    feature_order = [col_index.get_loc(name) for name in feature_names]\n",
    "\n",
    "    # Add percentage values to labels\n",
    "    percent_labels = [f\"{mapped_name} ({rel_importance[i]:.1f}%)\"\n",
    "                      for mapped_name, i in zip(mapped_feature_names, feature_order)]\n",
    "    ax.set_yticklabels(percent_labels, fontsize=10)\n",
    "\n",
    "    # Save updated plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{res_dir}/{combo}_shap_summary_plot_with_percentages.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # Return top 2 most important features (by mean absolute SHAP)\n",
    "    top2_indices = np.argsort(mean_abs_shap)[-2:][::-1]\n",
    "    top2_features = final_test_data.columns[top2_indices].tolist()\n",
    "    return top2_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def plot_pdp_across_runs(best_model, res_dir, all_test_data, feature_names=None, interaction_pair=None, colors=None, title=None):\n",
    "    \"\"\"\n",
    "    Plots PDPs with mean and std across multiple test sets for each feature.\n",
    "    Optionally adds an interaction plot.\n",
    "\n",
    "    Parameters:\n",
    "        best_model: trained model\n",
    "        all_test_data: list of pd.DataFrames used for PDP evaluation\n",
    "        feature_names: list of features to plot (default: all features in data)\n",
    "        interaction_pair: tuple of two features to plot interaction PDP\n",
    "        colors: optional color list\n",
    "    \"\"\"\n",
    "    if colors is None:\n",
    "        colors = [\"#22223B\", \"#4A4E69\", \"#9A8C98\", \"#C9ADA7\", \"#F2E9E4\"]\n",
    "\n",
    "    final_test_data = pd.concat(all_test_data, ignore_index=True)\n",
    "\n",
    "    if feature_names is None:\n",
    "        feature_names = final_test_data.columns.tolist()\n",
    "    \n",
    "    # Optional: map feature names to preferred display names\n",
    "    name_mapping = {\n",
    "        \"avg_alcmost\": \"Peer Drinking Amount\",\n",
    "        \"groupAtt_alc\": \"Peer Attitudes: Alcohol\",\n",
    "        \"avg_alcmost_freq\": \"Peer Drinking Frequency\",\n",
    "        \"alc_norm_5_r\": \"Perceived Peer Pressure\",\n",
    "        \"groupAtt_binge\": \"Peer Attitudes: Binges\"\n",
    "    }\n",
    "    display_names = [name_mapping.get(name, name) for name in feature_names]\n",
    "\n",
    "\n",
    "    num_features = len(feature_names)\n",
    "    num_plots = num_features + (1 if interaction_pair else 0)\n",
    "    num_cols = 3\n",
    "    num_rows = -(-num_plots // num_cols)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 5, num_rows * 4))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, feature_name in enumerate(feature_names):\n",
    "        pdp_values = []\n",
    "        feature_values_list = []\n",
    "\n",
    "        for dat in all_test_data:\n",
    "            ax_dummy = plt.figure().add_subplot()\n",
    "            ax_dummy.set_visible(False)\n",
    "            pdp_display = PartialDependenceDisplay.from_estimator(best_model, dat, [feature_name], ax=ax_dummy)\n",
    "            plt.close(ax_dummy.figure)\n",
    "\n",
    "            pdp_x = pdp_display.lines_[0][0].get_xdata()\n",
    "            pdp_y = pdp_display.lines_[0][0].get_ydata()\n",
    "            pdp_values.append(pdp_y)\n",
    "            feature_values_list.append(pdp_x)\n",
    "\n",
    "        common_feature_values = np.linspace(min(map(min, feature_values_list)),\n",
    "                                            max(map(max, feature_values_list)), num=100)\n",
    "\n",
    "        interpolated_pdp_values = []\n",
    "        for i in range(len(pdp_values)):\n",
    "            f_interp = interp1d(feature_values_list[i], pdp_values[i], kind=\"linear\", fill_value=\"extrapolate\")\n",
    "            interpolated_pdp_values.append(f_interp(common_feature_values))\n",
    "\n",
    "        pdp_values = np.array(interpolated_pdp_values)\n",
    "        pdp_mean = np.mean(pdp_values, axis=0)\n",
    "        pdp_std = np.std(pdp_values, axis=0)\n",
    "\n",
    "        ax = axes[idx]\n",
    "        ax.plot(common_feature_values, pdp_mean, label=\"Mean PDP\", color=colors[0], lw=2)\n",
    "        ax.fill_between(common_feature_values, pdp_mean - pdp_std, pdp_mean + pdp_std,\n",
    "                        color=colors[2], alpha=0.5, label=\"Std Dev\")\n",
    "        ax.set_ylabel(\"Predicted Value\")            \n",
    "        ax.set_title(f\"PDP for {display_names[idx]}\")\n",
    "        ax.legend()\n",
    "\n",
    "    if interaction_pair:\n",
    "        ax = axes[num_features]\n",
    "        PartialDependenceDisplay.from_estimator(best_model, final_test_data,\n",
    "                                                [interaction_pair], ax=ax)\n",
    "        name_a = name_mapping.get(interaction_pair[0], interaction_pair[0])\n",
    "        name_b = name_mapping.get(interaction_pair[1], interaction_pair[1])\n",
    "        ax.set_title(f\"Interaction: {name_a} & {name_b}\")\n",
    "\n",
    "\n",
    "    for i in range(num_plots, len(axes)):\n",
    "        fig.delaxes(axes[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Build filename\n",
    "    interaction_suffix = f\"_{interaction_pair[0]}_{interaction_pair[1]}\" if interaction_pair else \"\"\n",
    "    if not title:\n",
    "        filename = f\"{res_dir}/pdp_plots{interaction_suffix}.png\"\n",
    "    else:\n",
    "        filename = f\"{res_dir}/pdp_plots{title}.png\"\n",
    "    # Save figure\n",
    "    plt.savefig(filename, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "\n",
    "def run_rf_train_test(dataframes, param_grid, eval_metrics, outer_reps=50, k=5, CV_reps=5, model_choice_metric='f1', \n",
    "                      res_dir=f\"./results/\", model_type='xgb', test_set=0.3, resampling=None, permutation=False):\n",
    "\n",
    "    timestamp = int(time.time())\n",
    "    res_dir = f\"{res_dir}/{timestamp}_{model_type}_outer{outer_reps}_cvrep{CV_reps}_k{k}_{model_choice_metric}_testsize{test_set}_resampling{resampling}_perm{permutation}/\"\n",
    "    os.makedirs(res_dir, exist_ok=True)\n",
    "    \n",
    "    keys = list(dataframes.keys())\n",
    "\n",
    "    # combine data categories\n",
    "    combinations_keys = list(chain.from_iterable(combinations(keys, r) for r in range(1, 3)))\n",
    "    combo_validation_scores = {}\n",
    "    combo_test_scores = {}\n",
    "    best_models = {} \n",
    "    best_shap_vals = {}\n",
    "    best_paramses = {}\n",
    "\n",
    "    all_val_scores = {}\n",
    "    all_test_scores = {}\n",
    "    all_models_sub = []\n",
    "\n",
    "    for combo in tqdm(combinations_keys):\n",
    "        validation_scores = {metric: [] for metric in eval_metrics}\n",
    "        test_scores = {metric: [] for metric in eval_metrics}\n",
    "        merged_df = dataframes[combo[0]].copy()\n",
    "        top_models_group_sub = []\n",
    "        \n",
    "        for key in combo[1:]:\n",
    "            merged_df = merged_df.merge(dataframes[key].copy(), how='inner', on=['id', TARGET_VAR])\n",
    "        if TARGET_VAR not in merged_df.columns:\n",
    "            raise ValueError(f\"Target variable '{TARGET_VAR}' not found in merged dataframe for combo: {combo}\")\n",
    "    \n",
    "        all_shap_values = []\n",
    "        all_test_data = []\n",
    "        best_overall_score = -np.inf \n",
    "        best_model_for_combo = None\n",
    "        best_params_for_combo = None\n",
    "        best_shap_for_combo = None\n",
    "\n",
    "        for _ in range(outer_reps): # i repetitions of train/test\n",
    "\n",
    "            # Prepare train/test split for this i (random & stratified)\n",
    "            X_data, Y_data, X_test, Y_test = prepare_features_and_targets(merged_df.copy(), test_set=test_set, resampling=resampling)\n",
    "\n",
    "            # Shuffle labels for permutation tests\n",
    "            if permutation:\n",
    "                Y_data = Y_data.sample(frac=1, random_state=None).reset_index(drop=True)\n",
    "                Y_test = Y_test.sample(frac=1, random_state=None).reset_index(drop=True)\n",
    "\n",
    "            else:\n",
    "                best_model, best_scores, best_params = random_forest_kfold_grid_search(X_data, Y_data, \n",
    "                                                                                    param_grid, k=k, \n",
    "                                                                                    CV_reps=CV_reps, \n",
    "                                                                                    eval_metric=eval_metrics,\n",
    "                                                                                    model_choice_metric=model_choice_metric,\n",
    "                                                                                    res_dir=res_dir,\n",
    "                                                                                    model_type=model_type,\n",
    "                                                                                    combo=combo)\n",
    "            # Collect metrics\n",
    "            for metric, score in best_scores.items():\n",
    "                validation_scores[metric].append(score)\n",
    "\n",
    "            # Retrain the best model on the full training dataset and evaluate on the test set\n",
    "            best_model.fit(X_data, Y_data)\n",
    "            test_predictions = best_model.predict(X_test)\n",
    "            proba_predictions = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "            explainer = shap.TreeExplainer(best_model)\n",
    "            shap_values = explainer.shap_values(X_test) \n",
    "            shap_values = shap_values[:, :, 1]\n",
    "\n",
    "            # Append SHAP values and test data for later aggregation\n",
    "            all_shap_values.append(shap_values)\n",
    "            all_test_data.append(pd.DataFrame(X_test))\n",
    "\n",
    "            if best_scores[model_choice_metric] > best_overall_score:\n",
    "                best_overall_score = best_scores[model_choice_metric]\n",
    "                best_model_for_combo = best_model\n",
    "                best_params_for_combo = best_params\n",
    "                best_shap_for_combo = shap_values  # Store SHAP values if needed\n",
    "\n",
    "            if combo == ('group_sub',):\n",
    "                top_models_group_sub.append((best_scores[model_choice_metric], deepcopy(best_model)))\n",
    "\n",
    "            # Calculate and append metrics for the test set\n",
    "            test_scores = compute_test_metrics(Y_test, test_predictions, proba_predictions, test_scores)\n",
    "\n",
    "        # Keep track of the best model based on the model_choice_metric\n",
    "        if combo not in best_models or best_scores[model_choice_metric] > combo_validation_scores[combo][model_choice_metric]['mean']:\n",
    "            best_models[combo] = best_model_for_combo\n",
    "            joblib.dump(best_model_for_combo, f\"{res_dir}/model_{'_'.join(combo)}.joblib\")\n",
    "\n",
    "            best_shap_vals[combo] = best_shap_for_combo\n",
    "            best_paramses[combo] = best_params_for_combo\n",
    "\n",
    "            # Save top 10 models for group_sub combo\n",
    "            if combo == ('group_sub',):\n",
    "                top_models_group_sub = locals().get(\"top_models_group_sub\", [])\n",
    "                top_models_group_sub.append((best_overall_score, deepcopy(best_model_for_combo)))\n",
    "\n",
    "                # Sort and save top 10 by score\n",
    "                top_models_group_sub.sort(key=lambda x: x[0], reverse=True)\n",
    "                top10 = top_models_group_sub[:10]\n",
    "\n",
    "                subdir = os.path.join(res_dir, \"top10_group_sub_models\")\n",
    "                os.makedirs(subdir, exist_ok=True)\n",
    "\n",
    "                for i, (score, model) in enumerate(top10):\n",
    "                    joblib.dump(model, f\"{subdir}/model_rank{i+1}_score{score:.4f}.joblib\")\n",
    "\n",
    "                # Store back in locals so it's not overwritten each time\n",
    "                locals()[\"top_models_group_sub\"] = top_models_group_sub\n",
    "\n",
    "        top2_features = plot_shap_summary_with_percentages(all_shap_values, all_test_data, res_dir, combo)\n",
    "\n",
    "        plot_pdp_across_runs(\n",
    "            best_model=best_model_for_combo,\n",
    "            res_dir=res_dir,\n",
    "            all_test_data=all_test_data,\n",
    "            interaction_pair=tuple(top2_features)\n",
    "        )\n",
    "\n",
    "        # Calculate mean and 95% CI for validation scores\n",
    "        z = norm.ppf(0.975)  # 95% confidence level\n",
    "        final_validation_scores = {}\n",
    "        for metric, scores in validation_scores.items():\n",
    "            mean_score = np.mean(scores)\n",
    "            std_error = np.std(scores, ddof=1) / np.sqrt(len(scores))\n",
    "            ci_lower = mean_score - z * std_error\n",
    "            ci_upper = mean_score + z * std_error\n",
    "            final_validation_scores[metric] = {\n",
    "                'mean': mean_score,\n",
    "                '95%_CI': (ci_lower, ci_upper)\n",
    "            }\n",
    "        combo_validation_scores[combo] = final_validation_scores\n",
    "        all_val_scores[combo] = validation_scores\n",
    "        save_metrics_to_csv(all_val_scores, res_dir, 'all_val_scores.csv')\n",
    "\n",
    "        # Calculate mean and 95% CI for test scores\n",
    "        final_test_scores = {}\n",
    "        for metric, scores in test_scores.items():\n",
    "            mean_score = np.mean(scores)\n",
    "            std_error = np.std(scores, ddof=1) / np.sqrt(len(scores))\n",
    "            ci_lower = mean_score - z * std_error\n",
    "            ci_upper = mean_score + z * std_error\n",
    "            final_test_scores[metric] = {\n",
    "                'mean': mean_score,\n",
    "                '95%_CI': (ci_lower, ci_upper)\n",
    "            }\n",
    "        combo_test_scores[combo] = final_test_scores\n",
    "        all_test_scores[combo] = test_scores\n",
    "        save_metrics_to_csv(all_test_scores, res_dir, 'all_test_scores.csv')\n",
    "\n",
    "        # For validation scores\n",
    "        df_val = flatten_score_dict(combo_validation_scores, res_dir=res_dir, filename=\"validation_scores.csv\")\n",
    "        # For test scores\n",
    "        df_test = flatten_score_dict(combo_test_scores, res_dir=res_dir, filename=\"test_scores.csv\")\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_oos(test_df, res_dir, best_model, best_params, plot=True):\n",
    "    \n",
    "    # Drop common variables\n",
    "    test_features = test_df.drop(columns=[TARGET_VAR] + ['id'])\n",
    "    test_labels = test_df[TARGET_VAR]\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_proba = best_model.predict_proba(test_features)[:, 1]  # Probabilities for the positive class\n",
    "    y_pred = best_model.predict(test_features)\n",
    "    \n",
    "    # Compute metrics\n",
    "    tn, fp, fn, tp = confusion_matrix(test_labels, y_pred).ravel()\n",
    "    \n",
    "    scores = {\n",
    "        'auc': roc_auc_score(test_labels, y_pred_proba),\n",
    "        'f1': f1_score(test_labels, y_pred),\n",
    "        'accuracy': accuracy_score(test_labels, y_pred),\n",
    "        'specificity': tn / (tn + fp) if (tn + fp) > 0 else np.nan,\n",
    "        'sensitivity': recall_score(test_labels, y_pred),\n",
    "        'PPV': precision_score(test_labels, y_pred),\n",
    "        'NPV': tn / (tn + fn) if (tn + fn) > 0 else np.nan,\n",
    "        'MCC': matthews_corrcoef(test_labels, y_pred),\n",
    "        'balancedAcc': balanced_accuracy_score(test_labels, y_pred),\n",
    "        'pr_auc': roc_auc_score(test_labels, y_pred_proba),\n",
    "        'tn': tn,\n",
    "        'fn': fn,\n",
    "        'tp': tp,\n",
    "        'fp': fp\n",
    "    }\n",
    "    \n",
    "    colors = [\"#22223B\", \"#4A4E69\", \"#9A8C98\", \"#C9ADA7\", \"#F2E9E4\"]\n",
    "\n",
    "    # SHAP feature importance\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "    shap_values = explainer.shap_values(test_features)\n",
    "    shap_values = shap_values[:, :, 1]  # Extract SHAP values for positive class\n",
    "    \n",
    "    # Compute mean absolute SHAP values for importance ranking\n",
    "    shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "    feature_importance = pd.DataFrame({'feature': test_features.columns, 'importance': shap_importance})\n",
    "    feature_importance = feature_importance.sort_values(by='importance', ascending=False)\n",
    "    \n",
    "    if plot:\n",
    "        plot_shap_summary_with_percentages(\n",
    "            all_shap_values=[shap_values], \n",
    "            all_test_data=[pd.DataFrame(test_features)], \n",
    "            res_dir=res_dir,  # or pass a dynamic path\n",
    "            combo=\"test_oos\"\n",
    "        )\n",
    "        plot_pdp_across_runs(\n",
    "            best_model=best_model,\n",
    "            res_dir=res_dir,\n",
    "            all_test_data=[pd.DataFrame(test_features)],\n",
    "            interaction_pair=(\"avg_alcmost_freq\", \"avg_alcmost\"),\n",
    "            title=\"study_2\"\n",
    "        )\n",
    "            \n",
    "    return scores, best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    'demo': b5_demographic_response,\n",
    "    'alc_self': b1_alcohol_self_response,\n",
    "    'psych': b6_psychometric_response,\n",
    "    'group_sub': b2_group_subjective_response,\n",
    "    'group_socio': b3_group_sociometric_response,\n",
    "    'brain': b4_brain_response,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [50],\n",
    "    \"max_depth\": [3, 5],\n",
    "    \"min_samples_split\": [2, 4, 8],\n",
    "    \"min_samples_leaf\": [2, 3, 5]\n",
    "}\n",
    "\n",
    "eval_metrics = ['auc', 'f1', 'accuracy', 'specificity', 'sensitivity', 'PPV', 'NPV', 'MCC', 'balancedAcc', 'pr_auc', 'tn', 'fn', 'tp', 'fp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-fold CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normal Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_rf_train_test(\n",
    "    dataframes=dataframes,\n",
    "    param_grid=param_grid,\n",
    "    eval_metrics=eval_metrics,\n",
    "    outer_reps=100,\n",
    "    k=3,\n",
    "    CV_reps=5,\n",
    "    model_choice_metric='auc',\n",
    "    res_dir=\"./results_finalbuckets/\",\n",
    "    model_type='rf',\n",
    "    test_set=0.3,\n",
    "    resampling=None,\n",
    "    permutation=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sensitivities\n",
    "# run_rf_train_test(\n",
    "#     dataframes=dataframes,\n",
    "#     param_grid=param_grid,\n",
    "#     eval_metrics=eval_metrics,\n",
    "#     outer_reps=100,\n",
    "#     k=5,\n",
    "#     CV_reps=5,\n",
    "#     model_choice_metric='auc',\n",
    "#     res_dir=\"./results_finalbuckets/\",\n",
    "#     model_type='rf',\n",
    "#     test_set=0.3,\n",
    "#     resampling=None,\n",
    "#     permutation=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_rf_train_test(\n",
    "#     dataframes=dataframes,\n",
    "#     param_grid=param_grid,\n",
    "#     eval_metrics=eval_metrics,\n",
    "#     outer_reps=100,\n",
    "#     k=3,\n",
    "#     CV_reps=5,\n",
    "#     model_choice_metric='auc',\n",
    "#     res_dir=\"./results_finalbuckets/\",\n",
    "#     model_type='rf',\n",
    "#     test_set=0.4,\n",
    "#     resampling=None,\n",
    "#     permutation=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Permutation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_rf_train_test(\n",
    "#     dataframes=dataframes,\n",
    "#     param_grid=param_grid,\n",
    "#     eval_metrics=eval_metrics,\n",
    "#     outer_reps=100,\n",
    "#     k=3,\n",
    "#     CV_reps=5,\n",
    "#     model_choice_metric='auc',\n",
    "#     res_dir=\"./results_finalbuckets/\",\n",
    "#     model_type='rf',\n",
    "#     test_set=0.3,\n",
    "#     resampling=None,\n",
    "#     permutation=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Out-of-sample Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out-of sample testing\n",
    "res_dir = './results_finalbuckets/1747686299_rf_outer100_cvrep5_k3_auc_testsize0.3_resamplingNone_permFalse'\n",
    "loaded_model = joblib.load(\"./results_finalbuckets/1747686299_rf_outer100_cvrep5_k3_auc_testsize0.3_resamplingNone_permFalse/model_group_sub.joblib\")\n",
    "scores, best_params = test_oos(b2_group_subjective_test, res_dir, loaded_model, None, plot=True)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "from sklearn.utils import resample\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "def evaluate_top_models(res_dir, test_df, target_col='responsive', group_name=\"('group_sub',)\", \n",
    "                        top_n=10, n_iterations=10, desired_positive_rate=0.24, plot=False):\n",
    "    \"\"\"\n",
    "    Loads top N models from a result directory, resamples the test set with a desired positive rate, \n",
    "    and evaluates each model over multiple iterations.\n",
    "\n",
    "    Returns:\n",
    "        summary_df: DataFrame with mean and 95% CI for each metric.\n",
    "        all_scores_df: Raw scores from each resampling.\n",
    "    \"\"\"\n",
    "    # Load test metrics to identify top models\n",
    "    df_scores = pd.read_csv(os.path.join(res_dir, \"all_test_scores.csv\"))\n",
    "    group_df = df_scores[df_scores['group'] == group_name]\n",
    "\n",
    "    # Select top models based on AUC\n",
    "    top_indices = group_df['auc'].nlargest(top_n).index.tolist()\n",
    "\n",
    "    # Load corresponding models from saved files\n",
    "    top_models = []\n",
    "    model_dir = os.path.join(res_dir, \"top10_group_sub_models\")\n",
    "\n",
    "    for i in range(1, top_n + 1):\n",
    "        pattern = os.path.join(model_dir, f\"model_rank{i}_*.joblib\")\n",
    "        matched_files = glob.glob(pattern)\n",
    "        if matched_files:\n",
    "            top_models.append(joblib.load(matched_files[0]))\n",
    "\n",
    "    if len(top_models) == 0:\n",
    "        raise ValueError(f\"No models found\")\n",
    "\n",
    "    # Drop missing values\n",
    "    test_df = test_df.dropna()\n",
    "\n",
    "    # Split positives and negatives\n",
    "    positive_cases = test_df[test_df[target_col] == 1]\n",
    "    negative_cases = test_df[test_df[target_col] == 0]\n",
    "\n",
    "    all_scores = []\n",
    "\n",
    "    for model in tqdm(top_models, desc=\"Evaluating top models\"):\n",
    "        for _ in range(n_iterations):\n",
    "            # Stratified resampling\n",
    "            total_samples = len(test_df)\n",
    "            n_pos = int(total_samples * desired_positive_rate)\n",
    "            n_neg = total_samples - n_pos\n",
    "\n",
    "            pos_sample = resample(positive_cases, replace=True, n_samples=n_pos, random_state=None)\n",
    "            neg_sample = resample(negative_cases, replace=False, n_samples=n_neg, random_state=None)\n",
    "\n",
    "            balanced_df = pd.concat([pos_sample, neg_sample]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "            scores, _ = test_oos(balanced_df, res_dir, model, [], plot=plot)\n",
    "            all_scores.append(scores)\n",
    "\n",
    "    scores_df = pd.DataFrame(all_scores)\n",
    "\n",
    "    # Compute summary\n",
    "    mean_scores = scores_df.mean()\n",
    "    ci_lower, ci_upper = st.t.interval(0.95, df=len(scores_df)-1, loc=mean_scores, scale=scores_df.sem())\n",
    "\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Mean': mean_scores,\n",
    "        '95% CI Lower': ci_lower,\n",
    "        '95% CI Upper': ci_upper\n",
    "    })\n",
    "\n",
    "    return summary_df, scores_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "summary_df, all_scores_df = evaluate_top_models(\n",
    "    res_dir=\"/Users/fmagdalena/Documents/GitHub/shine-network-analysis/src/responsiveness/results_finalbuckets/1747686299_rf_outer100_cvrep5_k3_auc_testsize0.3_resamplingNone_permFalse\",\n",
    "    test_df=b2_group_subjective_test,\n",
    "    top_n=1,\n",
    "    n_iterations=100,\n",
    "    desired_positive_rate=0.23,\n",
    "    plot=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sensitivity \n",
    "# run_rf_train_test(\n",
    "#     dataframes=dataframes,\n",
    "#     param_grid=param_grid,\n",
    "#     eval_metrics=eval_metrics,\n",
    "#     outer_reps=100,\n",
    "#     k=3,\n",
    "#     CV_reps=5,\n",
    "#     model_choice_metric='auc',\n",
    "#     res_dir=\"./results_finalbuckets/\",\n",
    "#     model_type='rf',\n",
    "#     test_set=0.4,\n",
    "#     resampling=None,\n",
    "#     permutation=False\n",
    "# )\n",
    "\n",
    "# run_rf_train_test(\n",
    "#     dataframes=dataframes,\n",
    "#     param_grid=param_grid,\n",
    "#     eval_metrics=eval_metrics,\n",
    "#     outer_reps=100,\n",
    "#     k=5,\n",
    "#     CV_reps=5,\n",
    "#     model_choice_metric='auc',\n",
    "#     res_dir=\"./results_finalbuckets/\",\n",
    "#     model_type='rf',\n",
    "#     test_set=0.3,\n",
    "#     resampling=None,\n",
    "#     permutation=False\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
